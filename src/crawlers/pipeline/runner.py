from datetime import datetime
from urllib.parse import urlparse

from sqlalchemy import func, literal, select, tuple_

from src.crawlers.pipeline.types import ExtractedActivity
from src.crawlers.extractors.hardcoded import extract_from_event_page
from src.db.session import SessionLocal
from src.models.activity import Activity, FreeVerificationStatus, Source, Venue

_UPSERT_BATCH_SIZE = 500


def _to_free_status(value: str) -> FreeVerificationStatus:
    try:
        return FreeVerificationStatus(value)
    except ValueError:
        return FreeVerificationStatus.inferred


def _normalize_optional_text(value: str | None) -> str | None:
    if value is None:
        return None
    trimmed = value.strip()
    return trimmed or None


def _normalize_state(value: str | None) -> str | None:
    text = _normalize_optional_text(value)
    return text.upper() if text is not None else None


def _venue_key_for(
    venue_name: str | None,
    location_text: str | None,
    city: str | None,
    state: str | None,
) -> tuple[str, str | None, str | None] | None:
    normalized_name = _normalize_optional_text(venue_name)
    normalized_location = _normalize_optional_text(location_text)
    normalized_city = _normalize_optional_text(city)
    normalized_state = _normalize_state(state)
    if not normalized_name and not normalized_location:
        return None
    return (normalized_name or "Unknown Venue", normalized_city, normalized_state)


def _chunked(values: list[tuple], size: int) -> list[list[tuple]]:
    return [values[i : i + size] for i in range(0, len(values), size)]


def _resolve_venues(db, extracted: list[ExtractedActivity]) -> dict[tuple[str, str | None, str | None], Venue]:
    desired_venues: dict[tuple[str, str | None, str | None], str | None] = {}
    for item in extracted:
        venue_key = _venue_key_for(item.venue_name, item.location_text, item.city, item.state)
        if venue_key is None:
            continue
        address = _normalize_optional_text(item.location_text)
        if venue_key not in desired_venues or desired_venues[venue_key] is None:
            desired_venues[venue_key] = address

    if not desired_venues:
        return {}

    venue_names = list({key[0] for key in desired_venues})
    existing_venues: list[Venue] = []
    for names_chunk in _chunked([(name,) for name in venue_names], _UPSERT_BATCH_SIZE):
        names = [item[0] for item in names_chunk]
        existing_venues.extend(db.scalars(select(Venue).where(Venue.name.in_(names))).all())

    venues_by_key: dict[tuple[str, str | None, str | None], Venue] = {
        (venue.name, _normalize_optional_text(venue.city), _normalize_state(venue.state)): venue
        for venue in existing_venues
    }

    new_venues: list[Venue] = []
    for key, address in desired_venues.items():
        if key in venues_by_key:
            continue
        venue = Venue(
            name=key[0],
            address=address,
            city=key[1],
            state=key[2],
            website=None,
        )
        new_venues.append(venue)
        venues_by_key[key] = venue

    if new_venues:
        db.add_all(new_venues)
        db.flush()

    return venues_by_key


def upsert_extracted_activities(
    source_url: str,
    extracted: list[ExtractedActivity],
    *,
    adapter_type: str = "static_html",
) -> list[ExtractedActivity]:
    """Upsert extracted activity rows and return the deduplicated inputs."""
    deduped = list({(a.source_url, a.title, a.start_at): a for a in extracted}.values())
    if not deduped:
        return deduped

    now = datetime.utcnow()
    with SessionLocal() as db:
        source = db.scalar(
            select(Source)
            .where(literal(source_url).like(func.concat(Source.base_url, "%")))
            .order_by(func.length(Source.base_url).desc())
            .limit(1)
        )
        if source is None:
            parsed = urlparse(source_url)
            source = Source(
                name=(parsed.netloc or "unknown_source"),
                base_url=f"{parsed.scheme}://{parsed.netloc}" if parsed.scheme and parsed.netloc else source_url,
                adapter_type=adapter_type,
                crawl_frequency="daily",
                active=True,
            )
            db.add(source)
            db.flush()

        identity_keys = list({(a.source_url, a.title, a.start_at) for a in deduped})
        existing_items: list[Activity] = []
        for key_chunk in _chunked(identity_keys, _UPSERT_BATCH_SIZE):
            existing_items.extend(
                db.scalars(
                    select(Activity).where(
                        Activity.source_id == source.id,
                        tuple_(Activity.source_url, Activity.title, Activity.start_at).in_(key_chunk),
                    )
                ).all()
            )
        existing_by_key = {(a.source_url, a.title, a.start_at): a for a in existing_items}

        venues_by_key = _resolve_venues(db, deduped)

        for item in deduped:
            key = (item.source_url, item.title, item.start_at)
            current = existing_by_key.get(key)
            venue_key = _venue_key_for(item.venue_name, item.location_text, item.city, item.state)
            venue = venues_by_key.get(venue_key) if venue_key is not None else None
            if current is None:
                db.add(
                    Activity(
                        source_id=source.id,
                        source_url=item.source_url,
                        title=item.title,
                        description=item.description,
                        activity_type=item.activity_type,
                        age_min=item.age_min,
                        age_max=item.age_max,
                        drop_in=item.drop_in,
                        registration_required=item.registration_required,
                        start_at=item.start_at,
                        end_at=item.end_at,
                        timezone=item.timezone,
                        location_text=item.location_text,
                        venue_id=venue.id if venue else None,
                        free_verification_status=_to_free_status(item.free_verification_status),
                        first_seen_at=now,
                        last_seen_at=now,
                        updated_at=now,
                    )
                )
                continue

            current.description = item.description
            current.activity_type = item.activity_type
            current.age_min = item.age_min
            current.age_max = item.age_max
            current.drop_in = item.drop_in
            current.registration_required = item.registration_required
            current.end_at = item.end_at
            current.timezone = item.timezone
            current.location_text = item.location_text
            current.venue_id = venue.id if venue else None
            current.free_verification_status = _to_free_status(item.free_verification_status)
            current.last_seen_at = now
            current.updated_at = now

        db.commit()

    return deduped


async def run_single_page(source_url: str, html: str):
    """Ingest a single event page payload and upsert activities into MySQL.

    Current implementation keeps the extraction phase deterministic (hardcoded parser),
    then performs a lightweight upsert keyed by source_url/title/time fields.
    """
    # 1) Parse raw HTML into normalized activity objects.
    # The extractor returns a list because one page may contain multiple activities.
    extracted = extract_from_event_page(source_url=source_url, html=html)

    # 2) Persist with shared upsert logic used by other adapters.
    return upsert_extracted_activities(source_url=source_url, extracted=extracted, adapter_type="static_html")
